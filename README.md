**DATA ANALYST**
> Passionate about Data Science and Machine Learning, I leverage a strong foundation in Mathematics and Statistics to drive innovation in Artificial Intelligence. Currently a Data Analyst Executive at Lumina Datamatics wherein I apply advanced analytics to deliver actionable insights. Committed to staying at the forefront of technology, I seek opportunities to contribute to the evolving landscape of Machine Learning and Artificial Intelligence.

**EXPERIENCE**

> **Lumina DataMatics Limited**
> 
> Data Analyst Executive (August 2022 - Present)
> 
> _Roles and Responsibilities:_
> ➢ Successfully contributed to the Walmart-US’s Strategic programs project, leveraging my
> analytical skills and expertise in e-retail metrics.
> 
> ➢ Utilized Tableau to monitor and analyze key performance indicators (KPIs) related to e-retail
> metrics. Provided valuable insights and recommendations to optimize template settings for
> sellers, resulting in improved On-Time Delivery metrics.
> 
> ➢ Executed comprehensive data processing tasks, including data receiving, cleaning and
> preparation using Advanced Excel and My-SQL. Model building occurred after data preparation,
> which significantly raised seller metrics.
> 
> ➢ Managed the migration of Walmart sellers encountering shipping template issues to Automated
> Shipping Templates (AST), while implementing sanctions including the revocation of Two-Day
> and Pro seller badge privileges based on sellers' performance metrics.
> 
> ➢ Enhanced expedited Gross Merchandise Value (GMV) coverage and assortment along with the
> task of On time Delivery (OTD) improvement.
> 
> ➢ Achieved this by strategically balancing faster promise timelines with seller capabilities,
> resulting in improved operational efficiency and on-time deliveries.
> 
> ➢ Applied machine learning models, including Logistic Regression and Clustering, to categorize
> sellers effectively. These insights empowered clients to make informed decisions and take
> targeted actions.
> 
> ➢ Recognized for outstanding performance with the Quality Production (QPEP) award,
> showcasing consistent excellence in exceeding production targets for five consecutive months.
> 
> ➢ Proficiently utilized Salesforce, Tableau, Python, and Excel to streamline operations and
> provide actionable insights to the team and clients


>**August Analytics LLP**
>
> Data Analyst – Intern (February 2022 - July 2022)
>
>_Roles and Responsibilities:_
> ➢ Gained comprehensive understanding of the stock market and various financial derivatives
> through theoretical and practical study.
>
> ➢ Conducted daily market analysis, identifying trends and potential investment opportunities.
> ➢ Developed and implemented Excel-based valuation models for fundamental analysis, supporting
> informed investment decisions.
>
> ➢ Utilized an Excel-based optimizer and trading tool to determine optimal portfolio weights and
> execute trades efficiently.
>
> ➢ Leveraged discounted cash flow models and in-depth fundamental analysis to accurately value
> individual shares.
>
> ➢ Skills Acquired : Stock Management, Stock Control, Stock Market ,Financial Analysis, Financial
> Reporting, Data Analytics ,Advanced Excel


**EDUCATION**

> **Postgraduate Programme** (April 2023-April 2024)
> 
> Artificial Intelligence and Machine Learning
> 
> McCombs School of Business - The University of Texas at Austin
> 
> Grades(GPA) : 4.77
> 
> [E-portfolio](https://eportfolio.mygreatlearning.com/saif-merchant)



> **Bachelor of Science** (May 2019-May 2022)
> 
> Mathematics
> 
> SVKM’s Mithibai College
> 
> Grades(GPA) : 3.35

**LIBRARIES**
> • NumPy
> 
> • Scipy
> 
> • Pandas
> 
> • Scikit-learn
> 
> • Matplotlib
> 
> • Seaborn
> 
> • Plotly
> 
> • TensorFlow
> 
> • Keras, OpenCV, Scikit-Image,
> Python Pillow
> 
> • NLTK, Spacy

**Expertise**
> • Advanced Python
> 
> • Mathematics
> 
> • Advanced Excel
> 
> • Structured Query Language (My-SQL)
> 
> • Tableau-Data Viz
> 
> • Data Cleaning
> 
> • Data Preprocessing
> 
> • Data Visualization
> 
> • ML Model Development
> 
> • Bagging-Boosting Techniques
> 
> • Model Deployment
> 
> • Data Collection
> 
> • Computer Vision
> 
> • Natural Language Processing
> 
> • Artificial Neural Networks(ANN)
> 
> • Deep Learning Frameworks
>(TensorFlow, PyTorch)




**DATA ANALYTICAL PROJECTS EXPERIENCE**
> **Face Recognition**
> 
> _Course:Computer Vision (C.V)_
> 
> • Face recognition deals with Computer Vision a discipline of Artificial Intelligence and uses techniques
> of image processing and deep learning.
> 
> • The objective of this project is to build a face recognition system, which includes building a face
> detector to locate the position of a face in an image and a face identification model to recognize
> whose face it is by matching it to the existing database of faces.
> 
> • The success of a face recognition system heavily relies on the quality and diversity of the dataset
> used for training. Data collection involves gathering a large number of images containing faces.
> These images should cover various demographics, poses, expressions, and lighting conditions to
> ensure the model's robustness.
> 
> • In summary, building a face recognition system involves a combination of image processing, deep
> learning, data collection, model training, and deployment techniques to create a robust and reliable
> solution for identifying individuals from facial images.
> 
> • Skills & Tools Covered : TensorFlow, Keras, OpenCV, Siamese Networks, Triplet loss, CNN
>
> [Project Link]()

> **Statistical Natural Language Processing**
> 
> _Course:Natural Language Processing (NLP)_
> 
> • This project delivers 2 sub projects based on natural language processing and supervised learning
> classifier techniques.
> 
> • Part 1 Implementing a NLP based text classifier for a digital marketing company which can use input
> text parameters to determine the category based on blog’s literature from the corpus of numerous
> blogs curated and maintained.
> 
> • Part 2 Employing NLP text classifiers to curate and build a semi-rule based chat bot automation to be
> used as a technical support entity.
> 
> • Skills & Tools Covered : RNN, Text pre-processing, LSTM, Classification, TensorFlow, Word
> embedding
>
> [Project Link]()


> **Implementing a Image classification neural network to classify Street House View**
> **Numbers**
> 
> _Course: ArtificialNeural Networks(ANN)_
> 
> • SVHN is a real-world image dataset for developing object recognition algorithms with a requirement
> on data formatting but comes from a significantly harder, unsolved, real-world problem (recognizing
> digits and numbers in natural scene images).
> 
> • SVHN is obtained from house numbers in Google Street View images.
> 
> • The objective of the project was to learn how to implement a simple image classification pipeline
> based on the k-Nearest Neighbor and a deep neural network.
> 
> • Skills & Tools Covered : Neural Networks, Deep Learning, Keras, Image Recognition
>
> [Project Link]()

> **Online retail Orders Analysis**
> 
> _Course:Structured query language(SQL)_
> 
> • This project is based on the order management functionality of an online retail store in which you
> are provided with the “orders” database and you are asked some queries related to it.
> 
> • Answers to these queries will help the company in making data-driven decisions that will impact the
> overall growth of the online retail store.
> 
> • Skills & Tools Covered : Joins, Sub queries, SQL clause statement conditions, MYSQL workbench
>
> [Project Link]()

> **Feature Engineering & Model Tuning**
> 
> _Course:Featurization, Model Selection & Tuning_
> 
> • The project was accomplished by employing supervised learning, ensemble modeling, and unsupervised learning
> techniques to build and train a prediction model to identify Pass/Fail yield of a particular process entity for a
> semiconductor manufacturing company.
> 
> • This project helps to determine key factors contributing to yield excursions downstream in the process and will
> enable an increase in process throughput, decreased time to learn and reduce per-unit production costs.
> 
> • Skills & Tools Covered ;Supervised Learning, PCA, Feature Engineering, Model Tuning , Grid Search
>
> [Project Link]()

> **Classifying silhouettes of vehicles**
> 
> _Course:Unsupervised Learning_
> 
> • Classified vehicles into different types based on silhouettes which may be viewed from many angles.
> 
> • Used PCA in order to reduce dimensionality and SVM for classification.
> 
> • Skills & Tools Covered ; Support Vector Machines, Principal Component Analysis, Classification
>
>  [Project Link](https://github.com/SaifMerchant0304/Unsupervised-Learning)

> **Ensemble Techniques**
> 
> • Data cleaning and preprocessing
> 
> • Executing various EDA Visualizations.
> 
> • For determining the multiple ideal hyperparameters for each model, used Grid Search/Cross Validation.
> 
> • Making use of Decision Tree Classifier to create decision trees.
> 
> • Making use of Random Forest Classifier to create Random Forests.
> 
> • Making use of different Ensemble approaches, such as Bagging and Boosting (AdaBoost and Gradient Boost).
> 
> • Comparing every model to determine which one best fits our data.
> 
> • Skills & Tools Covered; EDA, Logistic regression, Decision Trees, Random forest, boosting
>
> [Project Link](https://github.com/SaifMerchant0304/Ensemble-Techniques)

> **Supervised Learning**
> 
> • Cleaning, pre-processing, performing univariate, bivariate, and multivariate visualizations;
> 
> • Constructing straightforward models utilizing linear and logistic regression models
> 
> • Accurate predictions were made using kNN/SVM Classifiers.
> 
> • To get the ideal values for k-Nearest Neighbors (kNN) and Support Vector Machine (SVM), Grid Search/Cross
> Validation was used.
> 
> • Contrast the model that fits our data the best.
> 
> • Skills & Tools Covered ; Logistic Regression, Naive Bayes, KNN, Classification, Python
>
> [Project Link](https://github.com/SaifMerchant0304/Supervised-Learning)

> **Data Preprocessing/Sanity Checks/EDA Techniques/Statistical**
> **Analysis/Multivariate Analysis**
> 
> _In the two different data sets that make up my project, I completed the following tasks:_
>
> • Conducted the requisite sanity tests
> 
> • Handled with the missing data
> 
> • Dealt with the potential outliers
> 
> • Performed Uni-variate, Bi-variate, Multivariate Analysis
>
> • Drawn conclusion based on the analysis
> 
> • Performed Statistical Operations and further Analysis
>
> [Project Link](https://github.com/SaifMerchant0304/Data-Preprocessing-Sanity-Checks-EDA-Techniques-Statistical-Analysis-and-Multivariate-Analysis)
 
> **Applied Statistics Project**
>
> _Part 1: Answering Industry Problems through Statistical Inferences_
> 
> * Identify industry problems and gather relevant data.
> 
> * Plot distributions and visualize data using histograms, box plots, etc.
>   
> * Formulate hypotheses and conduct hypothesis testing using t-tests, ANOVA, etc.
>   
> * Interpret results to address industry problems effectively.
>   
> _Part 2: Analyzing Past Tournament Information for Investment Decisions_
> 
> * Collect past tournament data including team performance and investment outcomes.
>   
> * Visualize trends and patterns using techniques like time series analysis and scatter plots.
>   
> * Apply statistical methods such as regression analysis to understand the relationship between performance and investment outcomes.
>   
> * Assess risks associated with investing in different teams and develop an investment strategy accordingly.
>   
> _Part 3: Analyzing Startup Battlefield Participants_
> 
> * Gather data on Startup Battlefield participants including team characteristics, funding rounds, and success/failure outcomes.
>   
> * Conduct descriptive analysis to understand startup characteristics.
>   
> * Compare successful and failed startups to identify common traits or patterns.
>   
> * Build predictive models to identify factors contributing to startup success or failure.
>   
> * Provide recommendations based on the analysis for aspiring entrepreneurs and investors.
>
>   [Project Link](https://github.com/SaifMerchant0304/Applied-Statistics-Project)  

> **Data Visualization using Tableau (Dashboard creation)**
> 
> • Average Price Analysis by Zipcode: Unveiling insights into pricing dynamics across zip code regions for targeted strategies.
> 
> • Bed Type Availability with City-wise Average Monthly Pricing: Revealing bed type availability and city-wise pricing insights for strategic decisions.
> 
> • Treemap Visualization for Airbnb Data: Providing a visually intuitive treemap representation for easy comprehension of key metrics and trends.
>
> [Project Link](https://public.tableau.com/app/profile/saif.merchant/viz/AirBnBFullProject_16578810581730/Dashboard1?publish=yes)

**CERTIFICATES**
> • Python For Everybody(Getting started with
> Python)
> 
_(University of Michigan)_
> 
> [credentials](https://coursera.org/share/b7207fd1f2559420f7c437651352591f)


> • Fundamentals of Visualization with Tableau
> 
> _(University of California, Davis)_
>
> [credentials](https://coursera.org/share/3cf9f7e1a06cc496be990059863ed02e)


> • Crash Course on Python- Google
> 
> _(Google)_
>
> [credentials](https://coursera.org/share/b68f1c2b6167babe03dfc1be796ef13b)


> • Data Data Everywhere-Google
> 
> _(Google)_
>
> [credentials](https://coursera.org/share/e734ca5aa9493ace64a57c5bec15ea48)


> • Data analysis using Excel
> 
> _(Rice University)_
>
> [credentials](https://coursera.org/share/2fb847f1dc5386bebdb692c88e7a314c)

> Python for Data Science
>
> _Great Learning_
>
> [credentials](https://olympus1.mygreatlearning.com/course_certificate/ZBIOOPRH)

**Soft Skills**
> • Communication Skills
> 
> • Problem-Solving Skills
> 
> • Critical Thinking
> 
> • Adaptability
> 
> • Creativity
> 
> • Time Management
> 
> • Attention to Detail
> 
> • Teamwork
>
> • Resilience
> 
> • Ethical Conduct
